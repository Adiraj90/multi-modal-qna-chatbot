# ğŸ¤– Multi-Modal QnA Chatbot

The **Multi-Modal QnA Chatbot** is an AI-powered web application built using **Streamlit** and **LangChain**.  
The main feature of this project is **multi-AI provider integration**, which means the chatbot is **not limited to a single AI model**. Users can switch between multiple AI providers such as **Groq, OpenAI, Google Gemini, Anthropic Claude, and Ollama (local models)** from the same interface.

The application supports real-world use cases like normal AI chat, context-aware conversations, internet-based QnA, document-based question answering, SQL database querying using natural language, and website content analysis.

---

## âœ¨ Features

- ğŸ” **Multi-AI Provider Support** (Groq, OpenAI, Gemini, Claude, Ollama)
- ğŸ¤– Basic AI Chat
- ğŸ§  Context-Aware Chat (remembers conversation)
- ğŸŒ Internet-Enabled QnA
- ğŸ“„ Chat with Documents (PDF, text files)
- ğŸ—„ï¸ Chat with SQL Databases using natural language
- ğŸŒ Chat with Website Content
- ğŸ¦™ Local AI support using Ollama (no API key required)

---

## ğŸ› ï¸ Tech Stack

- Python  
- Streamlit  
- LangChain  
- SQLite  
- Pandas  
- Multiple LLM Providers  

---

## ğŸ“ Project Structure
```bash
multi-modal-qna-chatbot/
|
â”œâ”€â”€ __pycache__
â”œâ”€â”€ .devcontainer
â”‚Â Â  â””â”€â”€ devcontainer.json
â”œâ”€â”€ .streamlit
â”‚Â Â  â”œâ”€â”€ config.toml
â”‚Â Â  â””â”€â”€ secrets.toml
â”œâ”€â”€ assets
â”‚Â Â  â””â”€â”€ Chinook.db 
â”œâ”€â”€ pages
â”‚Â Â  â”œâ”€â”€ 1_ğŸ¤– Basic Chatbot.py
â”‚Â Â  â”œâ”€â”€ 2_ğŸ§  Context-Aware Chatbot.py
â”‚Â Â  â”œâ”€â”€ 3_ğŸŒ Internet-Enabled Chatbot.py
â”‚Â Â  â”œâ”€â”€ 4_ğŸ“„ Chat with Your Documents.py
â”‚Â Â  â”œâ”€â”€ 5_ğŸ—„ï¸ Chat with SQL Database.py
â”‚Â Â  â””â”€â”€ 6_ğŸŒ Chat with Websites.py
â”œâ”€â”€ Home.py
â”œâ”€â”€ __init__.py
â”œâ”€â”€ chat_utils.py
â”œâ”€â”€ llm_providers.py
â”œâ”€â”€ pages_shared.py
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ streaming.py
â”œâ”€â”€ download_chinook.py
â””â”€â”€ tmp
```
---
## ğŸ¤– AI Providers

The chatbot is designed with **multi-provider AI support**, allowing users to switch between different large language models at runtime without changing the application logic.

### ğŸ”¹ OpenAI (Cloud)
- Requires an API key  
- Supports models like **GPT-3.5**, **GPT-4**, and **GPT-4o**  
- Strong reasoning and context handling  
- Best suited for production-grade conversations and complex queries  

### âš¡ Groq (Cloud)
- Requires an API key (free tier available)  
- Ultra-fast inference with low latency  
- Ideal for real-time responses and SQL generation  
- Recommended default provider for speed  

### ğŸŒ Google Gemini (Cloud)
- Requires an API key  
- Supports **Gemini Pro** and **Gemini 1.5** models  
- Good multimodal and analytical capabilities  

### ğŸ§  Anthropic Claude (Cloud)
- Requires an API key  
- Known for safer responses and strong reasoning  
- Useful for long-form and structured explanations  

### ğŸ¦™ Ollama (Local)
- Runs entirely on the local machine  
- No API key required  
- Supports models like **TinyLlama**, **Llama 3**, **Mistral**  
- Ideal for offline usage, testing, and privacy-focused workflows  

---

## ğŸ¨ Customization & Controls

### ğŸŒ¡ï¸ Temperature Control
The creativity of responses can be adjusted using the temperature parameter:

- **0.0** â†’ Highly precise and deterministic  
- **0.7** â†’ Balanced and natural conversations (default)  
- **1.0** â†’ More creative and diverse outputs  

### ğŸ§© Model Selection
- Choose different models within the same provider  
- Switch between providers in real time from the sidebar  
- No application restart required  

This flexibility allows users to experiment with multiple AI models and select the best one for their specific use case.

---

## ğŸš€ How to Run the Project

### 1ï¸âƒ£ Clone the Repository

```bash
git clone https://github.com/YOUR_USERNAME/multi-modal-qna-chatbot.git
cd multi-modal-qna-chatbot
```

### 2ï¸âƒ£ Create Virtual Environment

macOS / Linux

```bash
python3 -m venv venv
source venv/bin/activate
```
Windows
```bash 
python -m venv venv
venv\Scripts\activate
```
### 3ï¸âƒ£ Install Dependencies
```bash
pip install -r requirements.txt
```

### âœ… Create secrets.toml

Create the following file: .streamlit/secrets.toml

Add your API keys:
```toml
OPENAI_API_KEY = "your_openai_key"
GROQ_API_KEY = "your_groq_key"
GOOGLE_API_KEY = "your_google_key"
ANTHROPIC_API_KEY = "your_anthropic_key"
TAVILY_API_KEY = "your_tavily_key"
```
### ğŸš€ Running the Application

After cloning the repository and installing dependencies, the application can be started using:

```bash
streamlit run Home.py
```
---

## ğŸ’¡ Use Cases

- ğŸ” Ask questions across different AI models from a single interface  
- ğŸ“Š Query SQL databases without writing SQL manually  
- ğŸ“„ Extract insights from PDFs and text documents  
- ğŸŒ Analyze website content using AI  
- ğŸ§ª Compare responses from multiple LLM providers  
- ğŸ¦™ Run AI locally for privacy-sensitive or offline workflows

---

## ğŸ” Security & Sensitive Files

For security reasons, the following files **must not be pushed to GitHub**:

```text
.streamlit/secrets.toml
```
---

### 3ï¸âƒ£ **Why Multi-Provider Design**  
This is the **most important part for interviews**.

Add this section near the top (after Intro or Features):

```markdown
Most chatbots are tightly coupled to a single AI provider.  
This project is designed differently.

- Avoids vendor lock-in  
- Allows real-time provider switching  
- Enables cost and performance comparison  
- Improves reliability if one provider is unavailable  
- Supports both cloud and local AI models  

This architecture makes the chatbot flexible, future-proof, and production-ready.
```

## ğŸ‘¨â€ğŸ’» Author

**Aditya Raj**  
Computer Engineering Student  
Interests: Generative AI, LangChain, Applied AI Systems
